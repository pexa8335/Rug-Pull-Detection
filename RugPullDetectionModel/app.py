from flask import Flask, request, jsonify
import pandas as pd
import joblib
import numpy as np # Cáº§n cho LIME vÃ  predict_fn
import lime
import lime.lime_tabular
import os

app = Flask(__name__)

# Load model & scaler
MODEL_PATH = 'RugPullDetectionModel/isolation_forest_model_1.5.1.joblib'
SCALER_PATH = 'RugPullDetectionModel/scaler_1.5.1.pkl'

model = joblib.load(MODEL_PATH)
scaler = joblib.load(SCALER_PATH)

OPTIMAL_THRESHOLD = 0.0149 # found in code

# --- LIME Explainer Setup ---
# Cá»‘ gáº¯ng láº¥y tÃªn feature tá»« scaler
try:
    feature_names_for_lime = scaler.feature_names_in_.tolist()
except AttributeError:
    # Fallback náº¿u scaler Ä‘Æ°á»£c lÆ°u vá»›i phiÃªn báº£n scikit-learn cÅ© hÆ¡n
    # Hoáº·c náº¿u báº¡n biáº¿t chÃ­nh xÃ¡c sá»‘ lÆ°á»£ng vÃ  tÃªn features
    print("Warning: scaler.feature_names_in_ not available. Manually defining feature names based on notebook context.")
    # Dá»±a trÃªn notebook, X_train sau khi drop cÃ³ 19 cá»™t (cell 15)
    # ÄÃ¢y lÃ  má»™t giáº£ Ä‘á»‹nh, cáº§n kiá»ƒm tra láº¡i vá»›i notebook cá»§a báº¡n
    num_features_from_notebook = 19
    feature_names_for_lime = [f"feature_{i+1}" for i in range(num_features_from_notebook)]
    # VÃ­ dá»¥: ['TOTAL_ADDED_LIQUIDITY', ..., 'INACTIVITY_STATUS_Inactive']

# QUAN TRá»ŒNG: Dá»¯ liá»‡u huáº¥n luyá»‡n cho LIME
# Tá»T NHáº¤T: Táº£i dá»¯ liá»‡u huáº¥n luyá»‡n Ä‘Ã£ scale (X_scaled.values) tá»« notebook
# VÃ­ dá»¥: training_features_for_lime_values = np.load('X_scaled_values.npy')
# Náº¿u khÃ´ng cÃ³, chÃºng ta táº¡o dá»¯ liá»‡u dummy (Ã­t chÃ­nh xÃ¡c hÆ¡n cho LIME)
num_training_features = len(feature_names_for_lime)
# Táº¡o 100 máº«u dummy vá»›i phÃ¢n phá»‘i ngáº«u nhiÃªn, chá»‰ Ä‘á»ƒ LIME cÃ³ thá»ƒ cháº¡y
# Thay tháº¿ báº±ng dá»¯ liá»‡u huáº¥n luyá»‡n thá»±c táº¿ Ä‘Ã£ scale Ä‘á»ƒ cÃ³ giáº£i thÃ­ch tá»‘t nháº¥t
dummy_training_data_for_lime = np.random.rand(100, num_training_features)
print(f"LIME Explainer initialized with {num_training_features} features: {feature_names_for_lime[:5]}...")


class_names_for_lime = ['Anomaly', 'Normal'] # 0: Anomaly (-1 model), 1: Normal (1 model)

explainer = lime.lime_tabular.LimeTabularExplainer(
    training_data=dummy_training_data_for_lime, # Sá»¬ Dá»¤NG Dá»® LIá»†U HUáº¤N LUYá»†N THá»°C Táº¾ ÄÃƒ SCALE á» ÄÃ‚Y
    feature_names=feature_names_for_lime,
    class_names=class_names_for_lime,
    mode='classification', # VÃ¬ chÃºng ta Ä‘ang phÃ¢n loáº¡i Anomaly/Normal
    verbose=False,
    random_state=42 # Äá»ƒ cÃ³ thá»ƒ tÃ¡i táº¡o káº¿t quáº£
)

# HÃ m predict_fn cho LIME
def lime_predict_fn(X_lime_input_np):
    # X_lime_input_np lÃ  má»™t máº£ng numpy 2D
    # model.decision_function tráº£ vá» Ä‘iá»ƒm anomaly (Ä‘iá»ƒm tháº¥p hÆ¡n -> báº¥t thÆ°á»ng hÆ¡n)
    decision_scores = model.decision_function(X_lime_input_np)

    # Chuyá»ƒn Ä‘á»•i Ä‘iá»ƒm thÃ nh pseudo-probabilities cho [P(Anomaly), P(Normal)]
    # Isolation Forest: Ä‘iá»ƒm cao hÆ¡n lÃ  "normal" hÆ¡n.
    # Sigmoid cÃ³ thá»ƒ Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ squash Ä‘iá»ƒm vÃ o khoáº£ng (0,1)
    # P(Normal) ~ sigmoid(score). Äiá»ƒm = 0 lÃ  ranh giá»›i.
    prob_normal = 1 / (1 + np.exp(-decision_scores))
    prob_anomaly = 1 - prob_normal

    # LIME mong Ä‘á»£i Ä‘áº§u ra lÃ  (n_samples, n_classes)
    return np.vstack((prob_anomaly, prob_normal)).T
# --- End LIME Explainer Setup ---


@app.route('/')
def home():
    return "Liquidity Anomaly Detection API - Optimized Threshold with LIME"

@app.route('/predict', methods=['POST'])
def predict():
    try:
        data = request.get_json()
        if not data:
            return jsonify({"error": "No input data provided"}), 400

        # Sá»­ dá»¥ng feature_names_for_lime Ä‘Ã£ Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh á»Ÿ trÃªn
        # Äiá»u nÃ y Ä‘áº£m báº£o thá»© tá»± cá»™t nháº¥t quÃ¡n
        try:
            df = pd.DataFrame([data], columns=feature_names_for_lime)
        except ValueError as ve:
            return jsonify({"error": f"Input data error or incorrect columns: {str(ve)}"}), 400
        
        # Kiá»ƒm tra xem cÃ³ feature nÃ o bá»‹ thiáº¿u khÃ´ng
        for feature in feature_names_for_lime:
            if feature not in data:
                return jsonify({"error": f"Missing feature in input data: {feature}"}), 400
        
        # Äáº£m báº£o thá»© tá»± cá»™t Ä‘Ãºng nhÆ° lÃºc scaler Ä‘Æ°á»£c fit
        df = df[feature_names_for_lime]


        df_scaled = scaler.transform(df.values) # Truyá»n numpy array Ä‘á»ƒ trÃ¡nh warning

        anomaly_score = model.decision_function(df_scaled)
        score_value = anomaly_score[0]

        if score_value < OPTIMAL_THRESHOLD:
            prediction_label = -1 # Anomaly
            prediction_string = "Anomaly"
            if score_value < -0.1:
                 warning = "Anomaly - Highly Rug Pull Project! ğŸš¨"
            elif score_value < 0.0:
                 warning = "Anomaly - Suspicious Rug Pull Activity!"
            else:
                 warning = "Warning - Potential Anomaly Detected."
        else:
            prediction_label = 1 # Normal
            prediction_string = "Normal"
            warning = "Normal - Quite safe project."

        # --- LIME Explanation ---
        lime_explanation_list = []
        try:
            instance_to_explain_np = df_scaled[0] # Dá»¯ liá»‡u Ä‘Ã£ scale cá»§a má»™t máº«u
            # LIME cáº§n index cá»§a class (0 cho Anomaly, 1 cho Normal)
            predicted_class_index_lime = 0 if prediction_label == -1 else 1

            explanation = explainer.explain_instance(
                data_row=instance_to_explain_np,
                predict_fn=lime_predict_fn,
                num_features=10, # Sá»‘ lÆ°á»£ng feature muá»‘n hiá»ƒn thá»‹ trong giáº£i thÃ­ch
                labels=(predicted_class_index_lime,) # Giáº£i thÃ­ch cho lá»›p Ä‘Æ°á»£c dá»± Ä‘oÃ¡n
            )
            # Láº¥y giáº£i thÃ­ch dÆ°á»›i dáº¡ng danh sÃ¡ch cÃ¡c (feature, weight)
            lime_explanation_list = explanation.as_list(label=predicted_class_index_lime)
        except Exception as lime_e:
            app.logger.error(f"LIME explanation error: {str(lime_e)}")
            lime_explanation_list = [{"error": f"Could not generate LIME explanation: {str(lime_e)}"}]
        # --- End LIME Explanation ---

        return jsonify({
            "prediction_label_code": prediction_label,
            "prediction_label_string": prediction_string,
            "prediction_message": warning,
            "anomaly_score": float(score_value),
            "lime_explanation": lime_explanation_list # ThÃªm giáº£i thÃ­ch LIME
        })

    except KeyError as ke: # Máº·c dÃ¹ Ä‘Ã£ kiá»ƒm tra á»Ÿ trÃªn, Ä‘á»ƒ phÃ²ng há»
        return jsonify({"error": f"Missing feature in input data: {str(ke)}"}), 400
    except Exception as e:
        app.logger.error(f"Error during prediction: {str(e)}")
        return jsonify({"error": f"An unexpected error occurred: {str(e)}"}), 500


@app.route('/test-sample', methods=['GET'])
def test_sample():
    sample_input = {
        'TOTAL_ADDED_LIQUIDITY':  50000,
        'TOTAL_REMOVED_LIQUIDITY': 50000,
        'NUM_LIQUIDITY_ADDS': 5,
        'NUM_LIQUIDITY_REMOVES': 2,
        'ADD_TO_REMOVE_RATIO': 2.5,
        'LAST_POOL_ACTIVITY_TIMESTAMP_hour': 14,
        'LAST_POOL_ACTIVITY_TIMESTAMP_day': 20,
        'LAST_POOL_ACTIVITY_TIMESTAMP_weekday': 1,
        'LAST_POOL_ACTIVITY_TIMESTAMP_month': 5,
        'FIRST_POOL_ACTIVITY_TIMESTAMP_hour': 9,
        'FIRST_POOL_ACTIVITY_TIMESTAMP_day': 18,
        'FIRST_POOL_ACTIVITY_TIMESTAMP_weekday': 6,
        'FIRST_POOL_ACTIVITY_TIMESTAMP_month': 4,
        'LAST_SWAP_TIMESTAMP_hour': 13,
        'LAST_SWAP_TIMESTAMP_day': 20,
        'LAST_SWAP_TIMESTAMP_weekday': 1,
        'LAST_SWAP_TIMESTAMP_month': 5,
        'INACTIVITY_STATUS_Active': 1,
        'INACTIVITY_STATUS_Inactive': 0
    }
    
    # Äáº£m báº£o táº¥t cáº£ cÃ¡c feature_names_for_lime Ä‘á»u cÃ³ trong sample_input, náº¿u thiáº¿u thÃ¬ thÃªm giÃ¡ trá»‹ máº·c Ä‘á»‹nh (vÃ­ dá»¥ 0)
    for feature in feature_names_for_lime:
        if feature not in sample_input:
            sample_input[feature] = 0 # Hoáº·c má»™t giÃ¡ trá»‹ máº·c Ä‘á»‹nh há»£p lÃ½ khÃ¡c

    df = pd.DataFrame([sample_input], columns=feature_names_for_lime) # Sá»­ dá»¥ng feature_names_for_lime Ä‘á»ƒ Ä‘áº£m báº£o thá»© tá»±
    df = df[feature_names_for_lime] # Sáº¯p xáº¿p láº¡i cá»™t cho cháº¯c cháº¯n

    df_scaled = scaler.transform(df.values) # Truyá»n numpy array
    
    anomaly_score_test = model.decision_function(df_scaled)
    score_value_test = anomaly_score_test[0]

    if score_value_test < OPTIMAL_THRESHOLD:
        prediction_label_test = -1
        prediction_string_test = "Anomaly"
        if score_value_test < -0.1:
             warning_test = "Anomaly - Highly Rug Pull Project! ğŸš¨"
        elif score_value_test < 0.0:
             warning_test = "Anomaly - Suspicious Rug Pull Activity!"
        else:
             warning_test = "Warning - Potential Anomaly Detected."
    else:
        prediction_label_test = 1
        prediction_string_test = "Normal"
        warning_test = "Normal - Quite safe project."

    # --- LIME Explanation for test sample ---
    lime_explanation_list_test = []
    try:
        instance_to_explain_np = df_scaled[0]
        predicted_class_index_lime = 0 if prediction_label_test == -1 else 1
        explanation = explainer.explain_instance(
            data_row=instance_to_explain_np,
            predict_fn=lime_predict_fn,
            num_features=10,
            labels=(predicted_class_index_lime,)
        )
        lime_explanation_list_test = explanation.as_list(label=predicted_class_index_lime)
    except Exception as lime_e:
        app.logger.error(f"LIME explanation error for test sample: {str(lime_e)}")
        lime_explanation_list_test = [{"error": f"Could not generate LIME explanation: {str(lime_e)}"}]
    # --- End LIME Explanation ---

    return jsonify({
        "input_sample": sample_input,
        "prediction_label_code": prediction_label_test,
        "prediction_label_string": prediction_string_test,
        "prediction_message": warning_test,
        "anomaly_score": float(score_value_test),
        "lime_explanation": lime_explanation_list_test
    })


if __name__ == '__main__':
    # Thiáº¿t láº­p logging cÆ¡ báº£n cho Flask
    if not app.debug:
        import logging
        from logging.handlers import RotatingFileHandler
        file_handler = RotatingFileHandler('flask_app.log', maxBytes=1024 * 1024 * 100, backupCount=20)
        file_handler.setLevel(logging.ERROR)
        formatter = logging.Formatter("[%(asctime)s] {%(pathname)s:%(lineno)d} %(levelname)s - %(message)s")
        file_handler.setFormatter(formatter)
        app.logger.addHandler(file_handler)

    port = int(os.environ.get('PORT', 5000))
    app.run(host='0.0.0.0', port=port, debug=True) # debug=True há»¯u Ã­ch khi phÃ¡t triá»ƒn